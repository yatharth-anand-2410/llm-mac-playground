{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìö Preparing Data for Fine-Tuning\n",
                "\n",
                "To teach our Llama 3 model how to be a better assistant, we need to show it examples of perfect conversations. \n",
                "\n",
                "We are going to use the **`HuggingFaceH4/no_robots`** dataset. It contains 10,000 extremely high-quality instructions written by skilled human experts.\n",
                "\n",
                "First, let's download it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Downloading dataset...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c81b632022b34e799bab4c956eaff8dd",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "README.md: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "964c5b23e39549fc96a5dc58baa0f163",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "data/train-00000-of-00001.parquet:   0%|          | 0.00/10.5M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "15a9f573ed934b16a556dc408a0f669a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "data/test-00000-of-00001.parquet:   0%|          | 0.00/571k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e04195635f904381807c1cba376909e7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split:   0%|          | 0/9500 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "be28eeac8a3b4dd5801c1aa4b7907f0b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Example Raw Data ---\n",
                        "[\n",
                        "  {\n",
                        "    \"content\": \"Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert\\u2019s portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition\\u2014which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. \\u201cWe can\\u2019t ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,\\u201d says Rinkert.\",\n",
                        "    \"role\": \"user\"\n",
                        "  },\n",
                        "  {\n",
                        "    \"content\": \"Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.\",\n",
                        "    \"role\": \"assistant\"\n",
                        "  }\n",
                        "]\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "import json\n",
                "\n",
                "# 1. Download the dataset from Hugging Face\n",
                "print(\"Downloading dataset...\")\n",
                "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
                "\n",
                "# Let's look at one example from the training split\n",
                "example = dataset[\"train\"][0]\n",
                "print(\"\\n--- Example Raw Data ---\")\n",
                "print(json.dumps(example['messages'], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting the Data\n",
                "As we can see above, the raw dataset is just a list of dictionaries.\n",
                "\n",
                "But Llama 3 won't understand that during training! It demands that the text be formatted as a single long string, using its special `<|start_header_id|>` and `<|eot_id|>` tokens.\n",
                "\n",
                "Fortunately, the `mlx-lm` tokenizer we used in the first notebook has a built-in helper to do exactly this formatting!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca68c8410a2f4671b77c84555c0bc56f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8c24f51e18d34590a46cb1a78b4d84f2",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Fully formatted Llama 3 Training String ---\n",
                        "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
                        "\n",
                        "Please summarize the goals for scientists in this text:\n",
                        "\n",
                        "Within three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert‚Äôs portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition‚Äîwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. ‚ÄúWe can‚Äôt ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,‚Äù says Rinkert.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                        "\n",
                        "Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "from mlx_lm import load\n",
                "\n",
                "# 2. Load JUST the tokenizer (this is instant, it doesn't load the heavy model weights)\n",
                "model_id = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
                "_, tokenizer = load(model_id, tokenizer_config={\"trust_remote_code\": True})\n",
                "\n",
                "# 3. Create a function to convert the raw messages into Llama 3 format\n",
                "def format_for_llama3(messages):\n",
                "    # apply_chat_template takes the list of dictionaries and turns it into the exact training string\n",
                "    chat_string = tokenizer.apply_chat_template(messages, tokenize=False)\n",
                "    # mlx_lm expects the training data to have a key called 'text'\n",
                "    return {\"text\": chat_string}\n",
                "\n",
                "# Test the formatter on our example!\n",
                "formatted_example = format_for_llama3(example['messages'])\n",
                "print(\"\\n--- Fully formatted Llama 3 Training String ---\")\n",
                "print(formatted_example['text'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Saving the Dataset\n",
                "\n",
                "Now we just need to apply this formatting to 500 examples (10,000 would take hours to train, so we'll start small for practice!) and save it to a `.jsonl` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing 500 examples...\n",
                        "Processing 50 validation examples...\n",
                        "\n",
                        "‚úÖ Data preparation complete! You now have train.jsonl and valid.jsonl ready for fine-tuning.\n"
                    ]
                }
            ],
            "source": [
                "# 4. Process and save the data for training\n",
                "num_examples_to_use = 500\n",
                "\n",
                "print(f\"Processing {num_examples_to_use} examples...\")\n",
                "with open(\"train.jsonl\", \"w\") as f:\n",
                "    for i in range(num_examples_to_use):\n",
                "        messages = dataset[\"train\"][i][\"messages\"]\n",
                "        formatted_data = format_for_llama3(messages)\n",
                "        \n",
                "        # Write as a JSON Line (jsonl)\n",
                "        f.write(json.dumps(formatted_data) + \"\\n\")\n",
                "\n",
                "# We also need a small validation set so the model can test itself during training\n",
                "print(\"Processing 50 validation examples...\")\n",
                "with open(\"valid.jsonl\", \"w\") as f:\n",
                "    for i in range(50):\n",
                "        # We take these from the 'test' split so the model hasn't seen them before\n",
                "        messages = dataset[\"test\"][i][\"messages\"]\n",
                "        formatted_data = format_for_llama3(messages)\n",
                "        f.write(json.dumps(formatted_data) + \"\\n\")\n",
                "\n",
                "print(\"\\n‚úÖ Data preparation complete! You now have train.jsonl and valid.jsonl ready for fine-tuning.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
