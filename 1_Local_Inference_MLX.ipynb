{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ My First Local LLM on Mac M3\n",
        "Welcome to your local LLM playground! In this notebook, we will use Apple's `mlx-lm` library to download, load, and chat with a quantized Llama 3 model entirely natively on your M3 chip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Install necessary libraries\n",
        "# We'll install Apple's MLX library for LLMs.\n",
        "# !pip install mlx-lm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and loading mlx-community/Meta-Llama-3-8B-Instruct-4bit...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fcfed11cfba4dd1ac0414584fe0efaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ca220a4999e42e89ef7aedad4b587a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# 2. Load the model and tokenizer\n",
        "from mlx_lm import load, generate\n",
        "\n",
        "# We are using Meta's Llama-3 8B Instruct model.\n",
        "# Crucially, it is quantized to 4-bit precision so it fits easily into your 16GB memory (~4.5GB RAM usage).\n",
        "model_id = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
        "\n",
        "print(f\"Downloading and loading {model_id}...\")\n",
        "# Note: First time running this will download about 4-5GB of data. Subsequent runs will be instant.\n",
        "model, tokenizer = load(model_id)\n",
        "print(\"\\nâœ… Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating...\n",
            "==========\n",
            "Yay! Congratulations on loading your first local model!\n",
            "\n",
            "Okay, little buddy, let me explain \"quantization\" in a way that's easy to understand.\n",
            "\n",
            "Imagine you have a big box full of different colored crayons. Each crayon represents a special number that your model uses to make predictions or do tasks.\n",
            "\n",
            "Now, when your model is trained, it uses all these crayons (numbers) to learn and get better at its job. But, just like how you can't carry all those crayons with you when you go to school, your model can't use all those numbers at the same time.\n",
            "\n",
            "That's where quantization comes in! It's like taking a few of the most important crayons (numbers) and making smaller, simpler versions of them. This way, your model can still use them, but it's like having a smaller, more manageable box of crayons.\n",
            "\n",
            "Quantization makes your model smaller and faster, like a super-efficient crayon box! It's like a special trick that helps your model work better and use less computer power. And that's really cool!\n",
            "\n",
            "So, that's quantization in a nutshell (or a crayon box)! Do you have any more questions about it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm glad I could explain it in a way that's easy to understand. Do you have any more questions about quantization or would you like to know more about how it's used in machine learning models?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'm\n",
            "==========\n",
            "Prompt: 64 tokens, 93.257 tokens-per-sec\n",
            "Generation: 300 tokens, 7.000 tokens-per-sec\n",
            "Peak memory: 5.409 GB\n"
          ]
        }
      ],
      "source": [
        "# 3. Chat with the model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, smart AI assistant. Answer the user's questions clearly & concisely.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I just loaded my first local model on my Mac! Explain the concept of 'quantization' to me like I am 5 years old.\"}\n",
        "]\n",
        "\n",
        "# Different models expect prompts in different formats.\n",
        "# The tokenizer helps us format the system & user roles properly for Llama-3.\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages, \n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "# Generate the response!\n",
        "print(\"Generating...\")\n",
        "response = generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=prompt,\n",
        "    verbose=True,      \n",
        "    max_tokens=300    # <--- Changed this from temp to temperature!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating custom response...\n",
            "\n",
            "==========\n",
            "A question that has puzzled humans for centuries! While there is no definitive answer, I can provide some insights and perspectives that may help you find your own meaning in life.\n",
            "\n",
            "The meaning of life is a highly subjective and personal concept that can vary greatly from person to person. It's a question that has been debated by philosophers, scientists, and spiritual leaders throughout history. Some possible answers include:\n",
            "\n",
            "1. **Pursuit of happiness**: Living a life that brings joy, fulfillment, and contentment.\n",
            "2. **Self-discovery**: Understanding one's own values, passions, and purpose, and aligning with them.\n",
            "3. **Contribution to others**: Making a positive impact on the world, helping others, and leaving a lasting legacy.\n",
            "4. **Spiritual growth**: Connecting with a higher power, cultivating mindfulness, and finding inner peace.\n",
            "5. **Personal growth**: Developing one's skills, knowledge, and character, and becoming the best version of oneself.\n",
            "6. **Experiencing\n",
            "==========\n",
            "Prompt: 42 tokens, 55.470 tokens-per-sec\n",
            "Generation: 200 tokens, 7.038 tokens-per-sec\n",
            "Peak memory: 5.376 GB\n"
          ]
        }
      ],
      "source": [
        "# 2. Test out the fine-tuned model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, smart AI assistant. Answer the user's questions clearly & concisely.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages, \n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"Generating custom response...\\n\")\n",
        "response = generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=prompt,\n",
        "    verbose=True,      \n",
        "    max_tokens=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
