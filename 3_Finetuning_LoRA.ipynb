{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üèãÔ∏è‚Äç‚ôÄÔ∏è Fine-Tuning Llama 3 on your Mac!\n",
                "\n",
                "We have our dataset ready (`train.jsonl` and `valid.jsonl`). Now it's time to actually train the model! \n",
                "\n",
                "Because Apple Silicon shares its high-bandwidth memory between the CPU and GPU, your Mac is uniquely capable of training these models incredibly fast without needing a massive NVIDIA rig.\n",
                "\n",
                "### What is LoRA?\n",
                "We aren't going to \"Full Fine-tune\" the model. That takes massive amounts of memory. Instead, we use a technique called **LoRA** (Low-Rank Adaptation). \n",
                "\n",
                "With LoRA, we \"freeze\" the massive 8 Billion original weights of Llama 3 so they can't change. Instead, we train a tiny \"adapter\" layer over them. This adapter learns our new behavior and only takes up a few Megabytes of space, letting us train on your 16GB of RAM easily!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading pretrained model\n",
                        "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
                        "Fetching 6 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:00<00:00, 107088.61it/s]\u001b[A\n",
                        "Download complete: : 0.00B [00:00, ?B/s]              \n",
                        "Loading datasets\n",
                        "Training\n",
                        "Trainable parameters: 0.131% (10.486M/8030.261M)\n",
                        "mx.metal.device_info is deprecated and will be removed in a future version. Use mx.device_info instead.\n",
                        "Starting training..., iters: 200\n",
                        "Calculating loss...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [01:40<00:00,  4.01s/it]\n",
                        "Iter 1: Val loss 2.791, Val took 100.322s\n",
                        "Iter 10: Train loss 2.699, Learning Rate 1.000e-05, It/sec 0.113, Tokens/sec 38.518, Trained Tokens 3396, Peak mem 9.047 GB\n",
                        "Iter 20: Train loss 2.397, Learning Rate 1.000e-05, It/sec 0.141, Tokens/sec 38.389, Trained Tokens 6115, Peak mem 10.234 GB\n",
                        "Iter 30: Train loss 2.210, Learning Rate 1.000e-05, It/sec 0.127, Tokens/sec 39.523, Trained Tokens 9230, Peak mem 10.234 GB\n",
                        "Iter 40: Train loss 2.025, Learning Rate 1.000e-05, It/sec 0.132, Tokens/sec 37.713, Trained Tokens 12089, Peak mem 10.234 GB\n",
                        "Iter 50: Train loss 2.041, Learning Rate 1.000e-05, It/sec 0.125, Tokens/sec 39.299, Trained Tokens 15242, Peak mem 10.234 GB\n",
                        "Iter 60: Train loss 2.081, Learning Rate 1.000e-05, It/sec 0.129, Tokens/sec 39.442, Trained Tokens 18311, Peak mem 10.234 GB\n",
                        "Iter 70: Train loss 2.028, Learning Rate 1.000e-05, It/sec 0.208, Tokens/sec 39.746, Trained Tokens 20226, Peak mem 10.234 GB\n",
                        "Iter 80: Train loss 2.025, Learning Rate 1.000e-05, It/sec 0.167, Tokens/sec 40.005, Trained Tokens 22621, Peak mem 10.234 GB\n",
                        "Iter 90: Train loss 1.993, Learning Rate 1.000e-05, It/sec 0.120, Tokens/sec 40.257, Trained Tokens 25971, Peak mem 10.234 GB\n",
                        "Iter 100: Train loss 1.890, Learning Rate 1.000e-05, It/sec 0.149, Tokens/sec 40.028, Trained Tokens 28656, Peak mem 10.234 GB\n",
                        "Iter 100: Saved adapter weights to adapters/adapters.safetensors and adapters/0000100_adapters.safetensors.\n",
                        "Iter 110: Train loss 1.793, Learning Rate 1.000e-05, It/sec 0.137, Tokens/sec 38.495, Trained Tokens 31460, Peak mem 10.234 GB\n",
                        "Iter 120: Train loss 1.938, Learning Rate 1.000e-05, It/sec 0.127, Tokens/sec 39.970, Trained Tokens 34605, Peak mem 10.234 GB\n",
                        "Iter 130: Train loss 1.996, Learning Rate 1.000e-05, It/sec 0.199, Tokens/sec 40.016, Trained Tokens 36616, Peak mem 10.234 GB\n",
                        "libc++abi: terminating due to uncaught exception of type std::runtime_error: [METAL] Command buffer execution failed: Insufficient Memory (00000008:kIOGPUCommandBufferCallbackErrorOutOfMemory)\n"
                    ]
                }
            ],
            "source": [
                "# 1. Start the Training Run\n",
                "# We use a Jupyter 'magic' command (!) to run the terminal mlx_lm training script right inside this notebook cell.\n",
                "\n",
                "!mlx_lm.lora \\\n",
                "    --model \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\" \\\n",
                "    --train \\\n",
                "    --data . \\\n",
                "    --iters 200 \\\n",
                "    --batch-size 1 \\\n",
                "    --learning-rate 1e-5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## What just happened?\n",
                "\n",
                "If the cell above finished running, you just successfully fine-tuned an LLM! \n",
                "\n",
                "You'll notice it printed out a `loss` number every 10 iterations. That loss number represents how \"wrong\" the model was on our human-written training data. Over the 200 iterations, the loss should have trended downward as the model learned the new skills.\n",
                "\n",
                "### Where is the trained model?\n",
                "Because we used LoRA, it didn't overwrite the original Llama 3. Instead, it created a tiny new folder in this directory called **`adapters/`**. \n",
                "\n",
                "Inside that folder is a file named `adapters.safetensors` ‚Äì these are your newly trained matrix weights! Next, we can test out these new weights to see how they behave."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
