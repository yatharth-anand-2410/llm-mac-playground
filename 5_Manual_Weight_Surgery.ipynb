{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  Playing with Weights: Modifying the Math\n",
        "\n",
        "An AI model isn't magic; under the hood, it is just a massive collection of numbers arranged in grids called **Tensors** (or Matrices).\n",
        "\n",
        "Let's peel back the curtain, extract the raw mathematical weights of Llama 3, look at them, and then intentionally break the model by mathematically destroying its vocabulary!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.utils as mux\n",
        "from mlx_lm import load\n",
        "\n",
        "# 1. Load the model into memory\n",
        "print(\"Loading the model's brain...\")\n",
        "model_id = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
        "model, tokenizer = load(model_id)\n",
        "\n",
        "# The MLX model has two top-level parts:\n",
        "#   model.model  -> The transformer body (embeddings, attention layers, etc.)\n",
        "#   model.lm_head -> The final output projection (turns numbers back into words)\n",
        "print(\"\\nTop-level components:\", list(model.children().keys()))\n",
        "\n",
        "# Let's flatten all the parameters into a list so we can see them\n",
        "flat_params = mux.tree_flatten(model.parameters())\n",
        "\n",
        "print(\"\\n--- First 10 Weight Matrices in the Model ---\")\n",
        "for i, (name, parameter) in enumerate(flat_params[:10]):\n",
        "    shape = parameter.shape if hasattr(parameter, 'shape') else 'N/A'\n",
        "    dtype = parameter.dtype if hasattr(parameter, 'dtype') else 'N/A'\n",
        "    print(f\"  {i: >2}. {name: <50} Shape: {str(shape): <20} Dtype: {dtype}\")\n",
        "\n",
        "print(f\"\\n... and {len(flat_params)} total weight matrices in the model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Layout\n",
        "\n",
        "Notice two things above:\n",
        "- `model.embed_tokens.weight` has dtype `float16` and shape `[128256, 4096]` (regular floats!)\n",
        "- Most other layers have dtype `uint32` (these are **quantized** / compressed to save memory)\n",
        "\n",
        "The embedding table maps each of the 128,256 tokens to a vector of 4,096 numbers.\n",
        "\n",
        "Let's look at the exact numbers that define the word \"Apple\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "word = \"Apple\"\n",
        "token_id = tokenizer.encode(word)[0]\n",
        "\n",
        "# The embedding table is inside model.model (not directly on model)\n",
        "apple_weights = model.model.embed_tokens.weight[token_id]\n",
        "\n",
        "print(f\"Token ID for '{word}': {token_id}\")\n",
        "print(f\"Dtype: {apple_weights.dtype}\")\n",
        "print(f\"\\nThe mathematical representation of '{word}':\")\n",
        "print(apple_weights)\n",
        "print(f\"\\n(This array contains exactly {apple_weights.shape[0]} floating-point numbers)\")\n",
        "\n",
        "# Bonus: Let's compare it to another word to see they are completely different numbers\n",
        "word2 = \"Banana\"\n",
        "token_id2 = tokenizer.encode(word2)[0]\n",
        "banana_weights = model.model.embed_tokens.weight[token_id2]\n",
        "\n",
        "print(f\"\\n--- Comparing '{word}' vs '{word2}' ---\")\n",
        "print(f\"First 5 values of 'Apple':  {apple_weights[:5].tolist()}\")\n",
        "print(f\"First 5 values of 'Banana': {banana_weights[:5].tolist()}\")\n",
        "print(\"\\nCompletely different numbers = completely different meanings in the model's brain!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Doing Brain Surgery\n",
        "\n",
        "Since the embedding layer uses regular `float16` numbers (not quantized `uint32`), we can safely do math on it!\n",
        "\n",
        "Let's zero out the entire embedding table. This means *every single word* the model knows will become a meaningless zero-vector. The model will have no idea what any word means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlx_lm import generate\n",
        "\n",
        "def test_model(test_model_instance, label):\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    print(f\"\\nTesting {label}:\")\n",
        "    response = generate(test_model_instance, tokenizer, prompt=prompt, verbose=False, max_tokens=10)\n",
        "    print(f\"Response: '{response}'\")\n",
        "\n",
        "# 1. Test it while it's healthy\n",
        "test_model(model, \"Healthy Brain\")\n",
        "\n",
        "# 2. Perform brain surgery: Zero out the ENTIRE embedding table\n",
        "# This erases what every single word means!\n",
        "print(\"\\nPerforming brain surgery... zeroing out all 128,256 word embeddings...\")\n",
        "original_embeddings = model.model.embed_tokens.weight\n",
        "print(f\"Original embedding dtype: {original_embeddings.dtype}, shape: {original_embeddings.shape}\")\n",
        "\n",
        "# Create a zero matrix of the same shape and dtype\n",
        "model.model.embed_tokens.weight = mx.zeros_like(original_embeddings)\n",
        "\n",
        "# 3. Test the broken model\n",
        "test_model(model, \"Brain after erasing all word meanings\")\n",
        "\n",
        "print(\"\\nThe model can no longer understand words! It outputs garbage or nothing.\")\n",
        "print(\"This proves that the embedding weights ARE the model's vocabulary knowledge.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}