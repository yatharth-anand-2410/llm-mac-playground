{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§  Playing with Weights: Modifying the Math\n",
        "\n",
        "An AI model isn't magic; under the hood, it is just a massive collection of numbers arranged in grids called **Tensors** (or Matrices).\n",
        "\n",
        "Let's peel back the curtain, extract the raw mathematical weights of Llama 3, look at them, and then intentionally break the model by mathematically destroying its vocabulary!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading the model's brain...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1722dee03f264a15be52041585d00821",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee9881d5459b4789814656298061ea8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-level components: ['model', 'lm_head']\n",
            "\n",
            "--- First 10 Weight Matrices in the Model ---\n",
            "   0. model.embed_tokens.weight                          Shape: (128256, 4096)       Dtype: mlx.core.float16\n",
            "   1. model.layers.0.self_attn.q_proj.weight             Shape: (4096, 512)          Dtype: mlx.core.uint32\n",
            "   2. model.layers.0.self_attn.q_proj.scales             Shape: (4096, 64)           Dtype: mlx.core.float16\n",
            "   3. model.layers.0.self_attn.q_proj.biases             Shape: (4096, 64)           Dtype: mlx.core.float16\n",
            "   4. model.layers.0.self_attn.k_proj.weight             Shape: (1024, 512)          Dtype: mlx.core.uint32\n",
            "   5. model.layers.0.self_attn.k_proj.scales             Shape: (1024, 64)           Dtype: mlx.core.float16\n",
            "   6. model.layers.0.self_attn.k_proj.biases             Shape: (1024, 64)           Dtype: mlx.core.float16\n",
            "   7. model.layers.0.self_attn.v_proj.weight             Shape: (1024, 512)          Dtype: mlx.core.uint32\n",
            "   8. model.layers.0.self_attn.v_proj.scales             Shape: (1024, 64)           Dtype: mlx.core.float16\n",
            "   9. model.layers.0.self_attn.v_proj.biases             Shape: (1024, 64)           Dtype: mlx.core.float16\n",
            "\n",
            "... and 741 total weight matrices in the model!\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "import mlx.utils as mux\n",
        "from mlx_lm import load\n",
        "\n",
        "# 1. Load the model into memory\n",
        "print(\"Loading the model's brain...\")\n",
        "model_id = \"mlx-community/Meta-Llama-3-8B-Instruct-4bit\"\n",
        "model, tokenizer = load(model_id)\n",
        "\n",
        "# The MLX model has two top-level parts:\n",
        "#   model.model  -> The transformer body (embeddings, attention layers, etc.)\n",
        "#   model.lm_head -> The final output projection (turns numbers back into words)\n",
        "print(\"\\nTop-level components:\", list(model.children().keys()))\n",
        "\n",
        "# Let's flatten all the parameters into a list so we can see them\n",
        "flat_params = mux.tree_flatten(model.parameters())\n",
        "\n",
        "print(\"\\n--- First 10 Weight Matrices in the Model ---\")\n",
        "for i, (name, parameter) in enumerate(flat_params[:10]):\n",
        "    shape = parameter.shape if hasattr(parameter, 'shape') else 'N/A'\n",
        "    dtype = parameter.dtype if hasattr(parameter, 'dtype') else 'N/A'\n",
        "    print(f\"  {i: >2}. {name: <50} Shape: {str(shape): <20} Dtype: {dtype}\")\n",
        "\n",
        "print(f\"\\n... and {len(flat_params)} total weight matrices in the model!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Layout\n",
        "\n",
        "Notice two things above:\n",
        "- `model.embed_tokens.weight` has dtype `float16` and shape `[128256, 4096]` (regular floats!)\n",
        "- Most other layers have dtype `uint32` (these are **quantized** / compressed to save memory)\n",
        "\n",
        "The embedding table maps each of the 128,256 tokens to a vector of 4,096 numbers.\n",
        "\n",
        "Let's look at the exact numbers that define the word \"Apple\"!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token ID for 'Apple': 27665\n",
            "Dtype: mlx.core.float16\n",
            "\n",
            "The mathematical representation of 'Apple':\n",
            "array([-0.0201416, -0.000938416, -0.0100708, ..., -0.000482559, 0.00113678, 0.00567627], dtype=float16)\n",
            "\n",
            "(This array contains exactly 4096 floating-point numbers)\n",
            "\n",
            "--- Comparing 'Apple' vs 'Banana' ---\n",
            "First 5 values of 'Apple':  [-0.0201416015625, -0.00093841552734375, -0.01007080078125, -0.00102996826171875, -0.01190185546875]\n",
            "First 5 values of 'Banana': [-0.006072998046875, 0.01361083984375, 0.0003070831298828125, 0.005859375, -0.0023040771484375]\n",
            "\n",
            "Completely different numbers = completely different meanings in the model's brain!\n"
          ]
        }
      ],
      "source": [
        "word = \"Apple\"\n",
        "token_id = tokenizer.encode(word)[0]\n",
        "\n",
        "# The embedding table is inside model.model (not directly on model)\n",
        "apple_weights = model.model.embed_tokens.weight[token_id]\n",
        "\n",
        "print(f\"Token ID for '{word}': {token_id}\")\n",
        "print(f\"Dtype: {apple_weights.dtype}\")\n",
        "print(f\"\\nThe mathematical representation of '{word}':\")\n",
        "print(apple_weights)\n",
        "print(f\"\\n(This array contains exactly {apple_weights.shape[0]} floating-point numbers)\")\n",
        "\n",
        "# Bonus: Let's compare it to another word to see they are completely different numbers\n",
        "word2 = \"Banana\"\n",
        "token_id2 = tokenizer.encode(word2)[0]\n",
        "banana_weights = model.model.embed_tokens.weight[token_id2]\n",
        "\n",
        "print(f\"\\n--- Comparing '{word}' vs '{word2}' ---\")\n",
        "print(f\"First 5 values of 'Apple':  {apple_weights[:5].tolist()}\")\n",
        "print(f\"First 5 values of 'Banana': {banana_weights[:5].tolist()}\")\n",
        "print(\"\\nCompletely different numbers = completely different meanings in the model's brain!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Doing Brain Surgery\n",
        "\n",
        "Since the embedding layer uses regular `float16` numbers (not quantized `uint32`), we can safely do math on it!\n",
        "\n",
        "Let's zero out the entire embedding table. This means *every single word* the model knows will become a meaningless zero-vector. The model will have no idea what any word means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from mlx_lm import generate\n",
        "\n",
        "# def test_model(test_model_instance, label):\n",
        "#     messages = [\n",
        "#         {\"role\": \"system\", \"content\": \"You are a helpful, smart AI assistant. Answer the user's questions clearly & concisely.\"},\n",
        "#         {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "#     ]\n",
        "#     prompt = tokenizer.apply_chat_template(\n",
        "#         messages,\n",
        "#         tokenize=False,\n",
        "#         add_generation_prompt=True,\n",
        "#         max_tokens=20,\n",
        "#         verbose=True\n",
        "\n",
        "#     )\n",
        "#     print(f\"\\nTesting {label}:\")\n",
        "#     response = generate(test_model_instance, tokenizer, prompt=prompt, verbose=False, max_tokens=10)\n",
        "#     print(f\"Response: '{response}'\")\n",
        "\n",
        "# # 1. Test it while it's healthy\n",
        "# test_model(model, \"Healthy Brain\")\n",
        "\n",
        "# # 2. Perform brain surgery: Zero out the ENTIRE embedding table\n",
        "# # This erases what every single word means!\n",
        "# print(\"\\nPerforming brain surgery... zeroing out all 128,256 word embeddings...\")\n",
        "# original_embeddings = model.model.embed_tokens.weight\n",
        "# print(f\"Original embedding dtype: {original_embeddings.dtype}, shape: {original_embeddings.shape}\")\n",
        "\n",
        "# # Create a zero matrix of the same shape and dtype\n",
        "# model.model.embed_tokens.weight = mx.zeros_like(original_embeddings)\n",
        "\n",
        "# # 3. Test the broken model\n",
        "# test_model(model, \"Brain after erasing all word meanings\")\n",
        "\n",
        "# print(\"\\nThe model can no longer understand words! It outputs garbage or nothing.\")\n",
        "# print(\"This proves that the embedding weights ARE the model's vocabulary knowledge.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ORIGINAL lm_head values ===\n",
            "weight  (dtype: mlx.core.uint32, shape: (128256, 512))\n",
            "  First 10: [1351726744, 1180922772, 658999370, 2203367800, 1717855591, 716830345, 2186301553, 356746582, 1857407783, 1819494979]\n",
            "\n",
            "scales  (dtype: mlx.core.float16, shape: (128256, 64))\n",
            "  First 10: [0.004360198974609375, 0.0035076141357421875, 0.0073394775390625, 0.0036296844482421875, 0.00487518310546875, 0.00571441650390625, 0.004215240478515625, 0.004116058349609375, 0.0035724639892578125, 0.0041351318359375]\n",
            "\n",
            "biases  (dtype: mlx.core.float16, shape: (128256, 64))\n",
            "  First 10: [-0.02392578125, -0.0240478515625, -0.045654296875, -0.0283203125, -0.054931640625, -0.04345703125, -0.025634765625, -0.027587890625, -0.0281982421875, -0.031494140625]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import mlx.core as mx\n",
        "\n",
        "# ===== 1. Store the originals so we can restore them later =====\n",
        "original_weight = model.lm_head.weight\n",
        "original_scales = model.lm_head.scales\n",
        "original_biases = model.lm_head.biases\n",
        "\n",
        "# ===== 2. Print first 10 values of each (before surgery) =====\n",
        "print(\"=== ORIGINAL lm_head values ===\")\n",
        "print(f\"weight  (dtype: {original_weight.dtype}, shape: {original_weight.shape})\")\n",
        "print(f\"  First 10: {original_weight.reshape(-1)[:10].tolist()}\\n\")\n",
        "\n",
        "print(f\"scales  (dtype: {original_scales.dtype}, shape: {original_scales.shape})\")\n",
        "print(f\"  First 10: {original_scales.reshape(-1)[:10].tolist()}\\n\")\n",
        "\n",
        "print(f\"biases  (dtype: {original_biases.dtype}, shape: {original_biases.shape})\")\n",
        "print(f\"  First 10: {original_biases.reshape(-1)[:10].tolist()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MODIFIED lm_head (all weights reduced by 1000004) ===\n",
            "weight  First 10: [1350726740, 1179922768, 657999366, 2202367796, 1716855587, 715830341, 2185301549, 355746578, 1856407779, 1818494975]\n",
            "scales  First 10: [0.004360198974609375, 0.0035076141357421875, 0.0073394775390625, 0.0036296844482421875, 0.00487518310546875, 0.00571441650390625, 0.004215240478515625, 0.004116058349609375, 0.0035724639892578125, 0.0041351318359375]\n",
            "biases  First 10: [-0.02392578125, -0.0240478515625, -0.045654296875, -0.0283203125, -0.054931640625, -0.04345703125, -0.025634765625, -0.027587890625, -0.0281982421875, -0.031494140625]\n",
            "\n",
            "Generating with lm_head weights shift = 1000004...\n",
            "Response: 'The answer to -â€¦andâ€¦and//{{â€”aigtizmet'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ===== 3. Replace with random uint32 values =====\n",
        "# Pick any uint32 number you like! Change 42 to anything between 0 and 4294967295\n",
        "custom_value = 1000004\n",
        "\n",
        "model.lm_head.weight = (original_weight.astype(mx.int64) - custom_value).astype(mx.uint32)\n",
        "model.lm_head.scales = original_scales\n",
        "model.lm_head.biases = original_biases\n",
        "\n",
        "# ===== 4. Print first 10 values after surgery =====\n",
        "print(f\"=== MODIFIED lm_head (all weights reduced by {custom_value}) ===\")\n",
        "print(f\"weight  First 10: {model.lm_head.weight.reshape(-1)[:10].tolist()}\")\n",
        "print(f\"scales  First 10: {model.lm_head.scales.reshape(-1)[:10].tolist()}\")\n",
        "print(f\"biases  First 10: {model.lm_head.biases.reshape(-1)[:10].tolist()}\\n\")\n",
        "\n",
        "# ===== 5. Test the broken model =====\n",
        "from mlx_lm import generate\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful, smart AI assistant. Answer the user's questions clearly & concisely.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "]\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True,\n",
        "    max_tokens=20\n",
        ")\n",
        "print(f\"Generating with lm_head weights shift = {custom_value}...\")\n",
        "response = generate(model, tokenizer, prompt=prompt, verbose=False, max_tokens=10)\n",
        "print(f\"Response: '{response}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
